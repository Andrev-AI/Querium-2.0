{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder called: \"classifier\". Download the classifier from https://huggingface.co/nickmuchi/finbert-tone-finetuned-finance-topic-classification/tree/main\n",
    "Save everything inside a folder named \"classifier\". Download the following files:\n",
    "* config.json;\n",
    "* pytorch_model.bin;\n",
    "* special_tokens_map.json;\n",
    "* tokenizer_config.json;\n",
    "* tokenizer.json;\n",
    "* vocab.txt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 urllib3 selenium webdriver-manager transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, start_url, max_depth=3, max_pages=100, max_workers=10, selenium_workers=2, save_interval=10):\n",
    "        self.start_url = start_url\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages = max_pages\n",
    "        self.visited = set()\n",
    "        self.results = []\n",
    "        self.robot_parsers = {}\n",
    "        self.max_workers = max_workers\n",
    "        self.selenium_workers = selenium_workers\n",
    "        self.save_interval = save_interval\n",
    "        self.pages_crawled = 0\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "        self.session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        self.selenium_pool = []\n",
    "\n",
    "    def init_selenium(self):\n",
    "        options = Options()\n",
    "        options.add_argument(\"-headless\")\n",
    "        options.set_preference(\"network.cookie.cookieBehavior\", 0)  # Accept all cookies\n",
    "        driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "        return driver\n",
    "\n",
    "    def accept_cookies(self, driver):\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept') or contains(text(), 'Aceitar')]\"))\n",
    "            ).click()\n",
    "        except:\n",
    "            logging.info(\"No cookie acceptance button found or not clickable\")\n",
    "\n",
    "    def fetch_page(self, url, use_selenium=False):\n",
    "        if use_selenium:\n",
    "            driver = self.selenium_pool.pop()\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                self.accept_cookies(driver)\n",
    "                html = driver.page_source\n",
    "                return html\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Selenium request failed: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                self.selenium_pool.append(driver)\n",
    "        else:\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if response.status_code == 200:\n",
    "                    return response.text\n",
    "                else:\n",
    "                    return None\n",
    "            except requests.RequestException as e:\n",
    "                logging.error(f\"Request failed: {e}\")\n",
    "                return None\n",
    "\n",
    "    def parse_robots(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        if base_url not in self.robot_parsers:\n",
    "            robots_url = urljoin(base_url, \"/robots.txt\")\n",
    "            rp = RobotFileParser()\n",
    "            rp.set_url(robots_url)\n",
    "            try:\n",
    "                rp.read()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to read robots.txt: {e}\")\n",
    "            self.robot_parsers[base_url] = rp\n",
    "        return self.robot_parsers[base_url]\n",
    "\n",
    "    def is_allowed(self, url):\n",
    "        rp = self.parse_robots(url)\n",
    "        return rp.can_fetch(\"*\", url)\n",
    "\n",
    "    def extract_info(self, url, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title = soup.find('title').text.strip() if soup.find('title') else 'No Title'\n",
    "        text = ' '.join([p.text.strip() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])])\n",
    "        links = list(set([urljoin(url, a['href']) for a in soup.find_all('a', href=True)]))\n",
    "        \n",
    "        pub_time = None\n",
    "        meta_time = soup.find('meta', property='article:published_time')\n",
    "        if meta_time:\n",
    "            pub_time = meta_time['content']\n",
    "        else:\n",
    "            for name in ['pubdate', 'publishdate', 'timestamp', 'date']:\n",
    "                meta = soup.find('meta', attrs={'name': name})\n",
    "                if meta:\n",
    "                    pub_time = meta['content']\n",
    "                    break\n",
    "        \n",
    "        if pub_time:\n",
    "            try:\n",
    "                pub_time = datetime.fromisoformat(pub_time).isoformat()\n",
    "            except ValueError:\n",
    "                logging.warning(f\"Could not parse publication time: {pub_time}\")\n",
    "                pub_time = None\n",
    "\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'text': text,\n",
    "            'links': links,\n",
    "            'pub_time': pub_time,\n",
    "            'crawl_time': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    def save_results(self, filename='results.json'):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, indent=4, ensure_ascii=False)\n",
    "        logging.info(f\"Saved {len(self.results)} results to {filename}\")\n",
    "        self.results.clear()\n",
    "\n",
    "    def crawl(self, url, depth=0):\n",
    "        if depth > self.max_depth or len(self.visited) >= self.max_pages or url in self.visited:\n",
    "            return []\n",
    "\n",
    "        if not self.is_allowed(url):\n",
    "            return []\n",
    "\n",
    "        logging.info(f\"Crawling: {url}\")\n",
    "        self.visited.add(url)\n",
    "\n",
    "        use_selenium = False\n",
    "        html = self.fetch_page(url)\n",
    "        if not html or \"javascript required\" in html.lower():\n",
    "            use_selenium = True\n",
    "            html = self.fetch_page(url, use_selenium=True)\n",
    "\n",
    "        if html:\n",
    "            info = self.extract_info(url, html)\n",
    "            self.results.append(info)\n",
    "            self.pages_crawled += 1\n",
    "            \n",
    "            if self.pages_crawled % self.save_interval == 0:\n",
    "                self.save_results(f'partial_results_{self.pages_crawled}.json')\n",
    "            \n",
    "            return info['links']\n",
    "        \n",
    "        time.sleep(1)  # Respectful crawling\n",
    "        return []\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.selenium_workers):\n",
    "            self.selenium_pool.append(self.init_selenium())\n",
    "\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                future_to_url = {executor.submit(self.crawl, self.start_url): self.start_url}\n",
    "                while future_to_url and len(self.visited) < self.max_pages:\n",
    "                    for future in as_completed(future_to_url):\n",
    "                        url = future_to_url.pop(future)\n",
    "                        try:\n",
    "                            links = future.result()\n",
    "                            depth = urlparse(url).path.count('/')\n",
    "                            if depth < self.max_depth:\n",
    "                                for link in links:\n",
    "                                    if link not in self.visited and len(future_to_url) < self.max_workers:\n",
    "                                        future_to_url[executor.submit(self.crawl, link)] = link\n",
    "                        except Exception as exc:\n",
    "                            logging.error(f\"Exception for {url}: {exc}\")\n",
    "        finally:\n",
    "            for driver in self.selenium_pool:\n",
    "                driver.quit()\n",
    "            \n",
    "        self.save_results('final_results.json')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://example.com\"\n",
    "    crawler = Crawler(start_url, max_depth=10, max_pages=150, max_workers=6, selenium_workers=2, save_interval=30)\n",
    "    crawler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class AIIndexer:\n",
    "    def __init__(self, input_file='./final_results.json', output_file='alexandrya_ai.json', model_path='./classifier'):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.model_path = model_path\n",
    "        self.indexed_data = []\n",
    "\n",
    "        # Load AI model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.labels = [\n",
    "            \"Analyst Update\", \"Fed | Central Banks\", \"Company | Product News\",\n",
    "            \"Treasuries | Corporate Debt\", \"Dividend\", \"Earnings\", \"Energy | Oil\",\n",
    "            \"Financials\", \"Currencies\", \"General News | Opinion\", \"Gold | Metals | Materials\",\n",
    "            \"IPO\", \"Legal | Regulation\", \"M&A | Investments\", \"Macro\", \"Markets\",\n",
    "            \"Politics\", \"Personnel Change\", \"Stock Commentary\", \"Stock Movement\"\n",
    "        ]\n",
    "\n",
    "    def classify_text(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        scores = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "        \n",
    "        results = [{\"label\": label, \"score\": score} for label, score in zip(self.labels, scores)]\n",
    "        results = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def score_url(self, url):\n",
    "        score = 0\n",
    "        parsed_url = urlparse(url)\n",
    "        \n",
    "        # HTTPS\n",
    "        if parsed_url.scheme == 'https':\n",
    "            score += 5\n",
    "        \n",
    "        # URL length\n",
    "        score += min(len(url) // 10, 5)  # Max 5 points for length\n",
    "        \n",
    "        # Domain\n",
    "        domain = parsed_url.netloc\n",
    "        if domain.endswith('.com'):\n",
    "            score += 3\n",
    "        elif domain.endswith('.br'):\n",
    "            score += 4\n",
    "        elif domain.endswith('.gov'):\n",
    "            score += 5\n",
    "        elif domain.endswith('.edu'):\n",
    "            score += 5\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def index(self):\n",
    "        with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        for item in results:\n",
    "            classification_results = self.classify_text(item['text'])\n",
    "            url_score = self.score_url(item['url'])\n",
    "            \n",
    "            indexed_item = {\n",
    "                'url': item['url'],\n",
    "                'title': item['title'],\n",
    "                'text': item['text'],\n",
    "                'links': item['links'],  # Adicionando o campo 'links'\n",
    "                'classification_results': classification_results,\n",
    "                'url_score': url_score\n",
    "            }\n",
    "            self.indexed_data.append(indexed_item)\n",
    "        \n",
    "        with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.indexed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Indexing complete. Results saved to {self.output_file}\")\n",
    "\n",
    "    def search(self, query, top_n=5):\n",
    "        query_classification = self.classify_text(query)\n",
    "        top_query_label = query_classification[0]['label']\n",
    "        \n",
    "        results = []\n",
    "        for item in self.indexed_data:\n",
    "            item_top_label = item['classification_results'][0]['label']\n",
    "            if item_top_label == top_query_label:\n",
    "                results.append(item)\n",
    "        \n",
    "        # Sort results by the score of the matching label\n",
    "        results.sort(key=lambda x: next(r['score'] for r in x['classification_results'] if r['label'] == top_query_label), reverse=True)\n",
    "        \n",
    "        return results[:top_n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indexer = AIIndexer()\n",
    "    indexer.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "Some slow, will be resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, indexed_file='alexandrya_ai.json'):\n",
    "        with open(indexed_file, 'r', encoding='utf-8') as f:\n",
    "            self.indexed_data = json.load(f)\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.avg_doc_length = self.calculate_avg_doc_length()\n",
    "        self.pagerank_scores = self.calculate_pagerank()\n",
    "\n",
    "    def calculate_pagerank(self, damping_factor=0.85, num_iterations=100):\n",
    "        num_pages = len(self.indexed_data)\n",
    "        initial_value = 1.0 / num_pages\n",
    "        pagerank = {item['url']: initial_value for item in self.indexed_data}\n",
    "\n",
    "        url_to_index = {item['url']: i for i, item in enumerate(self.indexed_data)}\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            new_pagerank = {}\n",
    "            for item in self.indexed_data:\n",
    "                url = item['url']\n",
    "                incoming_pr = 0\n",
    "                for other_item in self.indexed_data:\n",
    "                    if 'links' in other_item and url in other_item['links']:\n",
    "                        incoming_pr += pagerank[other_item['url']] / len(other_item['links'])\n",
    "                new_pagerank[url] = (1 - damping_factor) / num_pages + damping_factor * incoming_pr\n",
    "\n",
    "            pagerank = new_pagerank\n",
    "\n",
    "        return pagerank\n",
    "\n",
    "    def calculate_avg_doc_length(self):\n",
    "        total_length = sum(len(self.tokenize(item['text'])) for item in self.indexed_data)\n",
    "        return total_length / len(self.indexed_data)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [token for token in tokens if token.isalnum() and token not in self.stop_words]\n",
    "\n",
    "    def compute_bm25_score(self, query, document, k1=1.5, b=0.75):\n",
    "        query_terms = self.tokenize(query)\n",
    "        doc_terms = self.tokenize(document)\n",
    "        doc_length = len(doc_terms)\n",
    "        term_freqs = Counter(doc_terms)\n",
    "        \n",
    "        score = 0\n",
    "        for term in query_terms:\n",
    "            if term in term_freqs:\n",
    "                idf = math.log((len(self.indexed_data) - len([d for d in self.indexed_data if term in self.tokenize(d['text'])])) + 0.5) - \\\n",
    "                      math.log(len([d for d in self.indexed_data if term in self.tokenize(d['text'])]) + 0.5)\n",
    "                tf = term_freqs[term]\n",
    "                numerator = tf * (k1 + 1)\n",
    "                denominator = tf + k1 * (1 - b + b * doc_length / self.avg_doc_length)\n",
    "                score += idf * numerator / denominator\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def get_snippet(self, text, query, snippet_length=200):\n",
    "        query_terms = set(self.tokenize(query))\n",
    "        words = text.split()\n",
    "        best_start = 0\n",
    "        max_matches = 0\n",
    "\n",
    "        for i in range(len(words) - snippet_length):\n",
    "            snippet = ' '.join(words[i:i+snippet_length])\n",
    "            matches = sum(1 for term in query_terms if term in self.tokenize(snippet))\n",
    "            if matches > max_matches:\n",
    "                max_matches = matches\n",
    "                best_start = i\n",
    "\n",
    "        return ' '.join(words[best_start:best_start+snippet_length]) + '...'\n",
    "\n",
    "    def search(self, query, top_n=5):\n",
    "        results = []\n",
    "        for item in self.indexed_data:\n",
    "            bm25_score = self.compute_bm25_score(query, item['text'])\n",
    "            url_score = item.get('url_score', 0)\n",
    "            pagerank_score = self.pagerank_scores.get(item['url'], 0)\n",
    "            \n",
    "            if 'classification_results' in item and item['classification_results']:\n",
    "                classification_score = item['classification_results'][0]['score']\n",
    "            else:\n",
    "                classification_score = 0\n",
    "            \n",
    "            total_score = (bm25_score * 0.4 + url_score * 0.1 + pagerank_score * 0.3 + classification_score * 0.2)\n",
    "            \n",
    "            snippet = self.get_snippet(item['text'], query)\n",
    "            \n",
    "            results.append({\n",
    "                'url': item['url'],\n",
    "                'title': item['title'],\n",
    "                'score': total_score,\n",
    "                'snippet': snippet\n",
    "            })\n",
    "        \n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return results[:top_n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_engine = SearchEngine()\n",
    "    query = \"\"\n",
    "    search_results = search_engine.search(query)\n",
    "    for result in search_results:\n",
    "        print(f\"URL: {result['url']}\")\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Score: {result['score']}\")\n",
    "        print(f\"Snippet: {result['snippet']}\")\n",
    "        print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
